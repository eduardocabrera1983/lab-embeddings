{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This lab is designed to help you solidify your understanding of embeddings by applying them to tasks like semantic similarity, clustering, and building a semantic search system.\n",
    "\n",
    "### Tasks:\n",
    "- Task 1: Semantic Similarity Comparison\n",
    "- Task 2: Document Clustering\n",
    "- Task 3: Enhance the Semantic Search System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Semantic Similarity Comparison\n",
    "### Objective:\n",
    "Compare semantic similarity between pairs of sentences using cosine similarity and embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Load a pre-trained Sentence Transformer model.\n",
    "2. Encode the sentence pairs.\n",
    "3. Compute cosine similarity for each pair.\n",
    "\n",
    "### Dataset:\n",
    "- \"A dog is playing in the park.\" vs. \"A dog is running in a field.\"\n",
    "- \"I love pizza.\" vs. \"I enjoy ice cream.\"\n",
    "- \"What is AI?\" vs. \"How does a computer learn?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Sentence 1: 'A dog is playing in the park.'\n",
      "  Sentence 2: 'A dog is running in a field.'\n",
      "  Cosine Similarity: 0.5220\n",
      "\n",
      "Pair 2:\n",
      "  Sentence 1: 'I love pizza.'\n",
      "  Sentence 2: 'I enjoy ice cream.'\n",
      "  Cosine Similarity: 0.5281\n",
      "\n",
      "Pair 3:\n",
      "  Sentence 1: 'What is AI?'\n",
      "  Sentence 2: 'How does a computer learn?'\n",
      "  Cosine Similarity: 0.3194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"A dog is playing in the park.\", \"A dog is running in a field.\"),\n",
    "    (\"I love pizza.\", \"I enjoy ice cream.\"),\n",
    "    (\"What is AI?\", \"How does a computer learn?\")\n",
    "]\n",
    "\n",
    "# Compute similarities\n",
    "\n",
    "#YOUR CODE HERE\n",
    "# Compute similarities for each pair\n",
    "for i, (sent1, sent2) in enumerate(sentence_pairs, 1):\n",
    "    # model.encode(): Converts text into numerical vectors (embeddings)\n",
    "    # These embeddings capture semantic meaning in high-dimensional space\n",
    "    embedding1 = model.encode([sent1])\n",
    "    embedding2 = model.encode([sent2])\n",
    "    \n",
    "    # cosine_similarity(): Measures angle between vectors (0 = perpendicular, 1 = identical)\n",
    "    # cosine similarity = (A ¬∑ B) / (|A| √ó |B|)\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    \n",
    "    print(f\"Pair {i}:\")\n",
    "    print(f\"  Sentence 1: '{sent1}'\")\n",
    "    print(f\"  Sentence 2: '{sent2}'\")\n",
    "    print(f\"  Cosine Similarity: {similarity:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Which sentence pairs are the most semantically similar? Why?\n",
    "- Can you think of cases where cosine similarity might fail to capture true semantic meaning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair2 is the sentece pair most semantically similar. \n",
    "Identical Syntactic Structure: Both follow \"I [positive emotion verb] [food noun]\"\n",
    "Semantic Parallel: \"love\" and \"enjoy\" are very close in embedding space\n",
    "Same Sentiment: Both express positive feelings\n",
    "Grammatical Similarity: Subject + verb + object pattern\n",
    "Concise Length: Shorter sentences often have higher similarity due to less noise\n",
    "\n",
    "Key Insights from These Results\n",
    "üîç What This Reveals About Embeddings:\n",
    "\n",
    "1. Syntactic Structure Matters A LOT\n",
    "Similar sentence patterns score higher than semantic similarity alone\n",
    "\"I love X\" vs. \"I enjoy Y\" has strong structural similarity\n",
    "\n",
    "\n",
    "2. Exact Word Matching Is Important\n",
    "\"Dog\" appearing in both sentences helps significantly\n",
    "Direct lexical overlap boosts similarity scores\n",
    "\n",
    "\n",
    "3. Sentence Length Effects\n",
    "Shorter sentences (Pair 2) can achieve higher similarity\n",
    "Less opportunity for divergent words to reduce similarity\n",
    "\n",
    "\n",
    "4. Question Type Distinctions\n",
    "Models distinguish between different types of questions\n",
    "\"What is\" vs. \"How does\" are structurally very different\n",
    "\n",
    "\n",
    "Examples where it might fail:\n",
    "1. Negation and Antonyms\n",
    "2. Sarcasm and Irony\n",
    "3. Context-Dependent Polysemy (Multiple Meanings)\n",
    "4. Temporal and Causal Relationships\n",
    "5. Implicit vs. Explicit Information\n",
    "6. Cultural References and Idioms\n",
    "7. Domain-Specific Jargon\n",
    "8. Quantity and Scale Differences\n",
    "9. Presupposition and Entailment\n",
    "10. Emotional Intensity Gradations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Document Clustering\n",
    "### Objective:\n",
    "Cluster a set of text documents into similar groups based on their embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Encode the documents using Sentence Transformers.\n",
    "2. Use KMeans clustering to group the documents.\n",
    "3. Analyze the clusters for semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents to cluster:\n",
      "  0: What is the capital of France?\n",
      "  1: How do I bake a chocolate cake?\n",
      "  2: What is the distance between Earth and Mars?\n",
      "  3: How do I change a flat tire on a car?\n",
      "  4: What is the best way to learn Python?\n",
      "  5: How do I fix a leaky faucet?\n",
      "\n",
      "Document embeddings shape: (6, 384)\n",
      "Each document is represented as a 384-dimensional vector\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Documents to cluster\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\"\n",
    "]\n",
    "\n",
    "# Encode documents\n",
    "print(\"Documents to cluster:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"  {i}: {doc}\")\n",
    "print()\n",
    "\n",
    "# model.encode(): Converts all documents into vector representations\n",
    "doc_embeddings = model.encode(documents)\n",
    "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
    "print(f\"Each document is represented as a {doc_embeddings.shape[1]}-dimensional vector\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform KMeans clustering\n",
      "Done!\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Perform KMeans clustering\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# Perform KMeans clustering\n",
    "# KMeans: Groups similar data points together by minimizing within-cluster variance\n",
    "# n_clusters=3: We expect 3 semantic groups (geography, cooking/DIY, technology)\n",
    "# random_state=42: Ensures reproducible results\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(doc_embeddings)\n",
    "\n",
    "print(\"Perform KMeans clustering\")\n",
    "print(\"Done!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Group documents by cluster\n",
    "clusters = {}\n",
    "for doc_idx, cluster_id in enumerate(cluster_labels):\n",
    "    if cluster_id not in clusters:\n",
    "        clusters[cluster_id] = []\n",
    "    clusters[cluster_id].append((doc_idx, documents[doc_idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 2:\n",
      "  - What is the capital of France?\n",
      "  - What is the best way to learn Python?\n",
      "\n",
      "Cluster 0:\n",
      "  - How do I bake a chocolate cake?\n",
      "  - What is the distance between Earth and Mars?\n",
      "\n",
      "Cluster 1:\n",
      "  - How do I change a flat tire on a car?\n",
      "  - How do I fix a leaky faucet?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print cluster assignments\n",
    "\n",
    "#YOUR CODE HERE\n",
    "# Print cluster assignments\n",
    "for cluster_id, cluster_docs in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for doc_idx, doc_text in cluster_docs:\n",
    "        print(f\"  - {doc_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many clusters make the most sense? Why?\n",
    "- Examine the documents in each cluster. Are they semantically meaningful? Can you assign a semantic \"theme\" to each cluster?\n",
    "- Try this exercise with a larger dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the Original 6-Document Dataset: 3 Clusters is optimal\n",
    "Why 3 clusters works best:\n",
    "\n",
    "Natural Semantic Groupings: The documents naturally fall into 3 thematic categories:\n",
    "\n",
    "DIY/How-to cluster: \"bake cake\", \"change tire\", \"fix faucet\"\n",
    "Factual/Knowledge cluster: \"capital of France\", \"distance Earth-Mars\"\n",
    "Technology/Learning cluster: \"learn Python\"\n",
    "\n",
    "\n",
    "2. Yes, very meaningful! Here are the semantic themes:\n",
    "Cluster 1: DIY/Practical Skills\n",
    "Cluster 2: Factual/Geographic Knowledge\n",
    "Cluster 3: Technology/Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Semantic Search System\n",
    "### Objective:\n",
    "Create a semantic search engine:\n",
    "A user provides a query and you search the dataset for semantically relevant documents to return. Return the top 5 results.\n",
    "\n",
    "### Dataset:\n",
    "- Use the following set of documents:\n",
    "    - \"What is the capital of France?\"\n",
    "    - \"How do I bake a chocolate cake?\"\n",
    "    - \"What is the distance between Earth and Mars?\"\n",
    "    - \"How do I change a flat tire on a car?\"\n",
    "    - \"What is the best way to learn Python?\"\n",
    "    - \"How do I fix a leaky faucet?\"\n",
    "    - \"What are the best travel destinations in Europe?\"\n",
    "    - \"How do I set up a local server?\"\n",
    "    - \"What is quantum computing?\"\n",
    "    - \"How do I build a mobile app?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search corpus contains 10 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extended documents dataset for search\n",
    "search_documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\",\n",
    "    \"What are the best travel destinations in Europe?\",\n",
    "    \"How do I set up a local server?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"How do I build a mobile app?\"\n",
    "]\n",
    "\n",
    "# Compute document embeddings\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "print(\"Search corpus contains\", len(search_documents), \"documents\\n\")\n",
    "\n",
    "# Compute document embeddings for the search corpus\n",
    "search_doc_embeddings = model.encode(search_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search function\n",
    "#This function should encode the user query and return the top N documents that most resemble it\n",
    "def semantic_search(query, documents, doc_embeddings, top_n=5):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \"\"\"\n",
    "    Performs semantic search using cosine similarity\n",
    "    \n",
    "    Parameters:\n",
    "    - query: User's search query (string)\n",
    "    - documents: List of documents to search through\n",
    "    - doc_embeddings: Pre-computed embeddings for documents\n",
    "    - top_n: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples: (similarity_score, document_index, document_text)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Encode the user query into the same embedding space\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate cosine similarity between query and all documents\n",
    "    # cosine_similarity returns a matrix, we take the first row\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Create list of (similarity, index, document) tuples\n",
    "    results = []\n",
    "    for i, similarity in enumerate(similarities):\n",
    "        results.append((similarity, i, documents[i]))\n",
    "    \n",
    "    # Sort by similarity score in descending order\n",
    "    # key=lambda x: x[0] means sort by the first element (similarity score)\n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return top N results\n",
    "    return results[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Explain programming languages.'\n",
      "----------------------------------------\n",
      "  1. [0.4352] What is quantum computing?\n",
      "  2. [0.3188] What is the best way to learn Python?\n",
      "  3. [0.1104] How do I build a mobile app?\n",
      "\n",
      "Query: 'How do I cook something sweet?'\n",
      "----------------------------------------\n",
      "  1. [0.5428] How do I bake a chocolate cake?\n",
      "  2. [0.2814] How do I set up a local server?\n",
      "  3. [0.2382] How do I change a flat tire on a car?\n",
      "\n",
      "Query: 'Tell me about space and planets.'\n",
      "----------------------------------------\n",
      "  1. [0.4337] What is the distance between Earth and Mars?\n",
      "  2. [0.2180] What is quantum computing?\n",
      "  3. [0.1801] What is the best way to learn Python?\n",
      "\n",
      "Query: 'I need help with car maintenance.'\n",
      "----------------------------------------\n",
      "  1. [0.2768] How do I change a flat tire on a car?\n",
      "  2. [0.1232] How do I build a mobile app?\n",
      "  3. [0.1182] What is the best way to learn Python?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the search function\n",
    "# Test the search function\n",
    "test_queries = [\n",
    "    \"Explain programming languages.\",\n",
    "    \"How do I cook something sweet?\",\n",
    "    \"Tell me about space and planets.\",\n",
    "    \"I need help with car maintenance.\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = semantic_search(query, search_documents, search_doc_embeddings, top_n=3)\n",
    "    \n",
    "    for rank, (similarity, doc_idx, doc_text) in enumerate(results, 1):\n",
    "        print(f\"  {rank}. [{similarity:.4f}] {doc_text}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What are the top-ranked results for the given queries?\n",
    "- How can you improve the ranking explanation for users?\n",
    "- Try this approach with a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANALYSIS AND INSIGHTS\n",
      "==================================================\n",
      "KEY CONCEPTS EXPLAINED:\n",
      "\n",
      "1. EMBEDDINGS:\n",
      "   - Convert text into numerical vectors that capture semantic meaning\n",
      "   - Similar texts have similar vector representations\n",
      "   - Enable mathematical operations on text\n",
      "\n",
      "2. COSINE SIMILARITY:\n",
      "   - Measures the cosine of the angle between two vectors\n",
      "   - Range: -1 (opposite) to 1 (identical)\n",
      "   - Ignores magnitude, focuses on direction/orientation\n",
      "   - Formula: cos(Œ∏) = (A¬∑B) / (|A|√ó|B|)\n",
      "\n",
      "3. KMEANS CLUSTERING:\n",
      "   - Partitions data into k clusters\n",
      "   - Minimizes within-cluster sum of squares\n",
      "   - Each point belongs to cluster with nearest centroid\n",
      "\n",
      "4. SEMANTIC SEARCH:\n",
      "   - Uses embeddings to find semantically similar content\n",
      "   - More powerful than keyword matching\n",
      "   - Can find relevant results even with different wording\n",
      "\n",
      "FUNCTIONS USED:\n",
      "- SentenceTransformer.encode(): Text ‚Üí Vector embeddings\n",
      "- cosine_similarity(): Compute similarity between vectors\n",
      "- KMeans.fit_predict(): Cluster data points\n",
      "- numpy operations: Array manipulation and sorting\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ANALYSIS AND INSIGHTS\n",
    "# ===============================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ANALYSIS AND INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"KEY CONCEPTS EXPLAINED:\")\n",
    "print()\n",
    "\n",
    "print(\"1. EMBEDDINGS:\")\n",
    "print(\"   - Convert text into numerical vectors that capture semantic meaning\")\n",
    "print(\"   - Similar texts have similar vector representations\")\n",
    "print(\"   - Enable mathematical operations on text\")\n",
    "print()\n",
    "\n",
    "print(\"2. COSINE SIMILARITY:\")\n",
    "print(\"   - Measures the cosine of the angle between two vectors\")\n",
    "print(\"   - Range: -1 (opposite) to 1 (identical)\")\n",
    "print(\"   - Ignores magnitude, focuses on direction/orientation\")\n",
    "print(\"   - Formula: cos(Œ∏) = (A¬∑B) / (|A|√ó|B|)\")\n",
    "print()\n",
    "\n",
    "print(\"3. KMEANS CLUSTERING:\")\n",
    "print(\"   - Partitions data into k clusters\")\n",
    "print(\"   - Minimizes within-cluster sum of squares\")\n",
    "print(\"   - Each point belongs to cluster with nearest centroid\")\n",
    "print()\n",
    "\n",
    "print(\"4. SEMANTIC SEARCH:\")\n",
    "print(\"   - Uses embeddings to find semantically similar content\")\n",
    "print(\"   - More powerful than keyword matching\")\n",
    "print(\"   - Can find relevant results even with different wording\")\n",
    "\n",
    "print(\"\\nFUNCTIONS USED:\")\n",
    "print(\"- SentenceTransformer.encode(): Text ‚Üí Vector embeddings\")\n",
    "print(\"- cosine_similarity(): Compute similarity between vectors\")  \n",
    "print(\"- KMeans.fit_predict(): Cluster data points\")\n",
    "print(\"- numpy operations: Array manipulation and sorting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
